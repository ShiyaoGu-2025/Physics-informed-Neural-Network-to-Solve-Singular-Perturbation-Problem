{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd4c8236",
   "metadata": {},
   "source": [
    "## Traditional Physics-Informed Neural Networks for 1D Singularly Perturbed Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n\nThis cell imports core libraries (NumPy, PyTorch, Matplotlib), selects `DEVICE` (GPU if available, else CPU), and sets the global default dtype to 64-bit floating point for higher precision.\nWe also set random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Imports & dtype/device ====\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.set_default_dtype(torch.float64)\n\nSEED = 1234\ntorch.manual_seed(SEED); np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Simple MLP\n\nWe define a vanilla fully connected neural network to approximate the solution $u_\\theta(x)$. \nThe network stacks `depth` Tanh-activated layers of `width` neurons and outputs a scalar.\nTanh is smooth and well-suited for approximating functions that require differentiability up to $u''(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42175727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Simple MLP (vanilla fully connected network)====\nclass MLP(nn.Module):\n    def __init__(self, in_dim=1, out_dim=1, width=64, depth=4):\n        super().__init__()\n        layers = [nn.Linear(in_dim, width), nn.Tanh()]\n        for _ in range(depth - 2):\n            layers += [nn.Linear(width, width), nn.Tanh()]\n        layers += [nn.Linear(width, out_dim)]\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics Residual\n\nGiven a callable $u(x)$ (which can be the raw network output or a hard-encoded trial function), this computes the residual of the 1D singularly perturbed ODE\n$$ -\\varepsilon\\,u''(x) + b(x)\\,u'(x) + c(x)\\,u(x) - f(x) = 0. $$\n\nWe use `torch.autograd` to obtain $u'(x)$ and $u''(x)$ with `create_graph=True` so the residual can be differentiated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb3faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== PDE residual (residual)====\n# u_fn: callable, u_fn(x) -> u(x); 这里兼容hard-encoding后的试函数or原始网络输出\ndef pde_residual(u_fn, x, eps):\n    x.requires_grad_(True)\n    u  = u_fn(x)\n    du = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n    d2u= torch.autograd.grad(du, x, torch.ones_like(du), create_graph=True)[0]\n    res = -eps * d2u + b_fun(x) * du + c_fun(x) * u - f_fun(x)\n    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundary Loss (Soft Constraints)\n\nThis section (soft BCs) defines the penalty terms for boundary conditions.\nIt is designed to support both Dirichlet $u(0)=\\alpha$, $u(1)=\\beta$ and Neumann $u'(0)=q_0$, $u'(1)=q_1$ conditions, adding them to the loss with suitable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Boundary loss (Boundary loss (soft constraint))====\n# Supports Dirichlet/Neumann boundary types\ndef boundary_loss(u_fn, nedge, bc_left, bc_right):\n    # 左端 x=0\n    x0 = torch.zeros(nedge//2, 1, device=DEVICE, dtype=torch.float64)\n    x0.requires_grad_(True)\n    u0  = u_fn(x0)\n    du0 = torch.autograd.grad(u0, x0, torch.ones_like(u0), create_graph=True)[0]\n\n    # 右端 x=1\n    x1 = torch.ones(nedge - nedge//2, 1, device=DEVICE, dtype=torch.float64)\n    x1.requires_grad_(True)\n    u1  = u_fn(x1)\n    du1 = torch.autograd.grad(u1, x1, torch.ones_like(u1), create_graph=True)[0]\n\n    def one_side_loss(kind_val, u, du):\n        kind, val = kind_val\n        kind = kind.lower()\n        if kind == 'dirichlet':\n            return torch.mean((u - float(val))**2)\n        elif kind == 'neumann':\n            return torch.mean((du - float(val))**2)\n        else:\n            raise ValueError(\"BC must be 'dirichlet' or 'neumann'.\")\n\n    return one_side_loss(bc_left, u0, du0) + one_side_loss(bc_right, u1, du1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard-encoding the Boundary Conditions\n\nWe construct a **trial function** of the form\n$$ u(x) = A(x) + S(x)\\,N_\\theta(x), $$\nwhere:\n- $A(x)$ satisfies the boundary conditions exactly.\n- $S(x)$ vanishes (value or derivative) at the corresponding end so that adding $S(x)N_\\theta(x)$ does not spoil the BCs satisfied by $A(x)$.\n\nThe code covers all four combinations of Dirichlet/Neumann at $x=0$ and $x=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Hard-encoding (hard-encoding)trial function construction for the four boundary combinations ====\n# Template： u(x) = A(x) + S(x) * N(x)\n# A(x) satisfies boundary conditions；S(x) vanishes (function or derivative) at the corresponding end without breaking the boundary conditions satisfied by A(x).\ndef build_hard_u(model, bc_left, bc_right):\n    kindL, valL = bc_left[0].lower(), float(bc_left[1])\n    kindR, valR = bc_right[0].lower(), float(bc_right[1])\n\n    def A_and_S(x):\n        # 方便广播\n        # 注意：x 是 (N,1) tensor\n        if kindL == 'dirichlet' and kindR == 'dirichlet':\n            # A(x) = α(1-x) + βx,  S(x) = x(1-x)\n            A = valL*(1.0 - x) + valR*x\n            S = x*(1.0 - x)\n\n        elif kindL == 'dirichlet' and kindR == 'neumann':\n            # A(x) = α + q1 * x^2/2,  S(x) = x(1-x)^2  (使 S(0)=0, S'(1)=0)\n            A = valL + valR * (x**2)/2.0\n            S = x*(1.0 - x)**2\n\n        elif kindL == 'neumann' and kindR == 'dirichlet':\n            # A(x) = q0*(x - x^2/2) + (β - q0/2)*x,  S(x) = x^2(1-x)\n            A = valL*(x - x**2/2.0) + (valR - 0.5*valL)*x\n            S = x**2*(1.0 - x)\n\n        elif kindL == 'neumann' and kindR == 'neumann':\n            # A'(0)=q0, A'(1)=q1；额外把常数规约 C=0 (否则解欠定)\n            # A(x) = q0*(x - x^2/2) + q1*(x^2/2),  S(x)=x^2(1-x)^2  (使 S'(0)=S'(1)=0)\n            A = valL*(x - x**2/2.0) + valR*(x**2/2.0)\n            S = (x**2)*((1.0 - x)**2)\n\n        else:\n            raise ValueError(\"BC must be combinations of 'dirichlet' or 'neumann'.\")\n\n        return A, S\n\n    def u_of_x(x):\n        A, S = A_and_S(x)\n        return A + S * model(x)\n\n    return u_of_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocation Sampling\n\nWe draw collocation points $x_i\\sim \\mathcal{U}(0,1)$ for enforcing the PDE residual in the interior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Collocation sampling (collocation sampling)====\ndef sample_collocation(ncol):\n    # uniform random sampling on (0,1)\n    return torch.rand(ncol, 1, device=DEVICE, dtype=torch.float64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for a Single $\\varepsilon$\n\nTrains the model for a given $\\varepsilon$ by minimizing a weighted sum of the interior (PDE) loss and boundary (BC) loss:\n$$ \\mathcal{L} = \\lambda_{\\text{PDE}}\\,\\big\\| r(x) \\big\\|_2^2 \\; + \\; \\lambda_{\\text{BC}}\\,\\big\\| g(\\text{BC}) \\big\\|_2^2, $$\nwhere $r(x)$ is the PDE residual and $g(\\text{BC})$ encodes boundary mismatches (or is zero if BCs are hard-encoded).\nReturns sampled $x$, predicted $u_\\theta(x)$, (optional) exact solution, and the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e4c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Train one ε ====\ndef train_one_eps(\n    eps, *, bc_left, bc_right, use_hard_bc,\n    ncol, nedge, epochs, lr, print_every, lam_bc, lam_pde,\n    seed=42\n):\n    torch.manual_seed(seed); np.random.seed(seed)\n\n    model = MLP(width=64, depth=4).to(DEVICE)\n    optim = torch.optim.Adam(model.parameters(), lr=lr)\n\n    # 选择 u(x) 的实现：hard-encoding or 软约束\n    if use_hard_bc:\n        u_fn = build_hard_u(model, bc_left, bc_right)\n        bc_loss_fn = (lambda: torch.tensor(0.0, device=DEVICE, dtype=torch.float64))  # hard-encoding无需边界损失\n    else:\n        u_fn = lambda x: model(x)\n        bc_loss_fn = (lambda: boundary_loss(u_fn, nedge, bc_left, bc_right))\n\n    hist = {'loss_pde': [], 'loss_bc': [], 'loss_total': []}\n\n    for ep in range(1, epochs + 1):\n        model.train()\n        x_col = sample_collocation(ncol)\n\n        # PDE residual\n        res = pde_residual(u_fn, x_col, eps)\n        loss_pde = torch.mean(res**2)\n\n        # 边界损失 (hard-encoding时为 0)\n        loss_bc = bc_loss_fn()\n\n        loss = lam_pde * loss_pde + lam_bc * loss_bc\n\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n\n        if ep % print_every == 0:\n            print(f\"[eps={eps:.2e}] epoch={ep:5d}  \"\n                  f\"loss_pde={loss_pde.item():.3e}  loss_bc={loss_bc.item():.3e}  total={loss.item():.3e}\")\n\n        hist['loss_pde'].append(loss_pde.item())\n        hist['loss_bc'].append(loss_bc.item())\n        hist['loss_total'].append(loss.item())\n\n    # 推理/可视化数据\n    model.eval()\n    with torch.no_grad():\n        x_plot = torch.linspace(0, 1, 400, device=DEVICE, dtype=torch.float64).view(-1,1)\n        u_pred = u_fn(x_plot).cpu().numpy().ravel()\n    x_plot_np = x_plot.cpu().numpy().ravel()\n\n    # 若两端是 Dirichlet，给出解析解对比\n    u_ex = None\n    \n    u_ex = exact_solution_dirichlet_dirichlet(x_plot_np, eps, alpha=float(bc_left[1]), beta=float(bc_right[1]))\n\n    return x_plot_np, u_pred, u_ex, hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweep Over a List of $\\varepsilon$ Values\n\nRuns training sequentially for each $\\varepsilon$ in a list (a simple homotopy/continuation strategy), collecting the curves for later visualization and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e844b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Helper to run a list of eps ====\ndef run_all_eps(\n    eps_list, *, bc_left, bc_right, use_hard_bc,\n    ncol, nedge, epochs, lr, print_every, lam_bc, lam_pde\n):\n    curves = []\n    for eps in eps_list:\n        x_np, u_pred, u_ex, hist = train_one_eps(\n            eps,\n            bc_left=bc_left, bc_right=bc_right, use_hard_bc=use_hard_bc,\n            ncol=ncol, nedge=nedge, epochs=epochs, lr=lr,\n            print_every=print_every, lam_bc=lam_bc, lam_pde=lam_pde\n        )\n        curves.append((eps, x_np, u_pred, u_ex))\n    return curves\n\ndef plot_curves(curves, title_suffix=\"\"):\n    plt.figure(figsize=(7.2, 4.6))\n    for eps, x_np, u_pred, u_ex in curves:\n        plt.plot(x_np, u_pred, '-', label=f'PINN pred, eps={eps:g}')\n        if u_ex is not None:\n            plt.plot(x_np, u_ex, '--', label=f'exact, eps={eps:g}')\n    plt.xlabel('x'); plt.ylabel('u(x)')\n    plt.title(f\"PINN for -ε u'' + b u' + c u = f on (0,1) {title_suffix}\")\n    plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Table Utility\n\nBuilds a common grid, interpolates predicted and exact solutions, and computes absolute and relative errors for easy comparison across different $\\varepsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Results table utility (skip if you already added it)====\ndef build_results_table(curves, grid_n: int = 201):\n    x_common = np.linspace(0.0, 1.0, grid_n)\n    data = {'x': x_common}\n    metrics_rows = []\n    for eps, x_np, u_pred, u_ex in curves:\n        u_pred_i = np.interp(x_common, x_np, u_pred)\n        data[f'pred[eps={eps:g}]'] = u_pred_i\n        if u_ex is not None:\n            u_ex_i = np.interp(x_common, x_np, u_ex)\n            data[f'exact[eps={eps:g}]'] = u_ex_i\n            abs_err = np.abs(u_pred_i - u_ex_i)\n            rel_err = abs_err / (np.maximum(np.abs(u_ex_i), 1e-12))\n            data[f'abs_err[eps={eps:g}]'] = abs_err\n            data[f'rel_err%[eps={eps:g}]'] = 100.0 * rel_err\n            l2_rel = np.linalg.norm(u_pred_i - u_ex_i) / (np.linalg.norm(u_ex_i) + 1e-12)\n            linf_rel = np.max(rel_err)\n            metrics_rows.append({\n                'eps': eps,\n                'L2_rel': float(l2_rel),\n                'Linf_rel': float(linf_rel),\n                'MAE': float(np.mean(abs_err)),\n                'RMSE': float(np.sqrt(np.mean(abs_err**2)))\n            })\n        else:\n            data[f'exact[eps={eps:g}]'] = np.nan\n            data[f'abs_err[eps={eps:g}]'] = np.nan\n            data[f'rel_err%[eps={eps:g}]'] = np.nan\n    df_table = pd.DataFrame(data)\n    df_metrics = pd.DataFrame(metrics_rows)\n    return df_table, df_metrics\n\ndef show_and_save_tables(df_table, df_metrics,\n                         table_path='pinn_results_table.csv',\n                         metrics_path='pinn_results_metrics.csv'):\n    from IPython.display import display\n    display(df_table)\n    if not df_metrics.empty:\n        print(\"\\n—— 指标汇总 (metrics)——\")\n        display(df_metrics)\n    df_table.to_csv(table_path, index=False)\n    if not df_metrics.empty:\n        df_metrics.to_csv(metrics_path, index=False)\n    print(f\"\\n已保存: {table_path}\")\n    if not df_metrics.empty:\n        print(f\"已保存: {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated Training Entry Point: `train(...)`\n\nA convenience wrapper that wires together the components above using global hyperparameters (sampling counts, epochs, learning rate, logging cadence, loss weights, and whether BCs are hard-encoded).\nIt orchestrates the full training loop over a specified list of $\\varepsilon$ values and produces plots and/or tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106083b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Integrated training API train(...) ====\nfrom typing import List, Tuple, Optional\nimport os\nimport pandas as pd\n\ndef train(EPS_LIST: List[float],\n          layer_side: str = 'right',\n          bc_left: Tuple[str, float] = ('dirichlet', 0.0),\n          bc_right: Tuple[str, float] = ('dirichlet', 1.0),\n          seed: int = 42):\n    \"\"\"\n    Unified training entry point (Integrated training entry-point)\n    - Uses global hyperparameters：NCOL, NEDGE, EPOCHS_PER_STAGE, LR, PRINT_EVERY, PLOT_EVERY,\n                    LAM_BC, lam_pde, USE_HARD_BC\n    - 训练每个 eps，按 PRINT_EVERY 打印；按 PLOT_EVERY 进行中途可视化 (可选)。\n    - 结束时：\n        1) 画一张包含所有 eps 的整合图，并保存为 'pinn_all_eps.png'\n        2) 生成“结果汇总表格”和“指标表”，保存为 CSV\n\n    返回:\n        curves: List[(eps, x_np, u_pred, u_ex or None)]\n        df_table: 逐点汇总表 (预测/真解/误差列)\n        df_metrics: 指标汇总表\n        fig_path: 最终整合图的文件路径\n    \"\"\"\n    # 读取全局超参数\n    global NCOL, NEDGE, EPOCHS_PER_STAGE, LR, PRINT_EVERY, PLOT_EVERY\n    global LAM_BC, lam_pde, USE_HARD_BC\n\n    torch.manual_seed(seed); np.random.seed(seed)\n\n    curves = []\n    # 逐个 eps 训练\n    for eps in EPS_LIST:\n        # 构建模型与优化器\n        model = MLP(width=64, depth=4).to(DEVICE)\n        optim = torch.optim.Adam(model.parameters(), lr=LR)\n\n        # u(x) 选择：hard-encoding or 软约束\n        if USE_HARD_BC:\n            u_fn = build_hard_u(model, bc_left, bc_right)\n            def bc_loss_fn():\n                return torch.tensor(0.0, device=DEVICE, dtype=torch.float64)\n        else:\n            u_fn = lambda x: model(x)\n            def bc_loss_fn():\n                return boundary_loss(u_fn, NEDGE, bc_left, bc_right)\n\n        # 训练循环\n        for ep in range(1, EPOCHS_PER_STAGE + 1):\n            model.train()\n            x_col = sample_collocation(NCOL)\n\n            # PDE residual损失 (residual loss)\n            res = pde_residual(u_fn, x_col, eps)\n            loss_pde = torch.mean(res**2)\n\n            # 边界损失 (hard-encoding为 0)\n            loss_bc = bc_loss_fn()\n\n            loss = lam_pde * loss_pde + LAM_BC * loss_bc\n\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n\n            # 打印 (PRINT_EVERY)\n            if ep % PRINT_EVERY == 0:\n                print(f\"[eps={eps:.2e}] epoch={ep:5d}  \"\n                      f\"loss_pde={loss_pde.item():.3e}  \"\n                      f\"loss_bc={loss_bc.item():.3e}  total={loss.item():.3e}\")\n\n            # 中途可视化 (PLOT_EVERY)，可设 PLOT_EVERY<=0 以关闭\n            if PLOT_EVERY and PLOT_EVERY > 0 and (ep % PLOT_EVERY == 0 or ep == EPOCHS_PER_STAGE):\n                model.eval()\n                with torch.no_grad():\n                    x_plot = torch.linspace(0, 1, 400, device=DEVICE, dtype=torch.float64).view(-1,1)\n                    u_pred = u_fn(x_plot).cpu().numpy().ravel()\n                x_plot_np = x_plot.cpu().numpy().ravel()\n\n                u_ex = None\n                \n                u_ex = exact_solution(\n                        x_plot_np, eps\n                    )\n\n                # 单 eps 小图 (不保存，只显示)\n                plt.figure(figsize=(6.4, 3.8))\n                plt.plot(x_plot_np, u_pred, '-', label=f'PINN pred (eps={eps:g}, ep={ep})')\n                if u_ex is not None:\n                    plt.plot(x_plot_np, u_ex, '--', label='exact')\n                plt.xlabel('x'); plt.ylabel('u(x)')\n                title_mode = 'hard' if USE_HARD_BC else 'soft'\n                plt.title(f'PINN ({title_mode})  eps={eps:g}  epoch={ep}')\n                plt.legend(); plt.tight_layout(); plt.show()\n\n        # 该 eps 的最终曲线 (用于整合图与表格)\n        model.eval()\n        with torch.no_grad():\n            x_plot = torch.linspace(0, 1, 400, device=DEVICE, dtype=torch.float64).view(-1,1)\n            u_pred = u_fn(x_plot).cpu().numpy().ravel()\n        x_plot_np = x_plot.cpu().numpy().ravel()\n\n        u_ex = None\n        if True:\n            u_ex = exact_solution(\n                x_plot_np, eps\n            )\n\n        curves.append((eps, x_plot_np, u_pred, u_ex))\n\n    # ======= 训练完成：整合图 =======\n   \n    plt.figure(figsize=(7.6, 4.8))\n    for eps, x_np, u_pred, u_ex in curves:\n        plt.plot(x_np, u_pred, '-', label=f'pred, eps={eps:g}')\n        if u_ex is not None:\n            plt.plot(x_np, u_ex, '--', label=f'exact, eps={eps:g}')\n    title_mode = 'hard' if USE_HARD_BC else 'soft'\n    plt.xlabel('x'); plt.ylabel('u(x)')\n    plt.title(f\"PINN ({title_mode})  layer_side={layer_side}  on (0,1)\")\n    plt.legend(); plt.tight_layout()\n    plt.show()\n    \n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c96c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== Hyperparameters (last cell) ===========================\n# Manually specify boundary-layer side and boundary conditions\nLAYER_SIDE = 'right'                  # 'left' or 'right'，set manually after inspection (unused in this implementation)\nBC_LEFT  = ('dirichlet', 0.0)        # ('dirichlet', α) or ('neumann', q0)\nBC_RIGHT = ('dirichlet', 1.0)        # ('dirichlet', β) or ('neumann', q1)\n\n# Training-related\nEPS_LIST = [ 1e-2, 5e-3, 1e-3, 1e-4]                  # e.g., could be changed to [5e-2, 1e-2, 5e-3, 1e-3]\nNCOL, NEDGE = 2048, 512\nEPOCHS_PER_STAGE = 2000\nPRINT_EVERY, PLOT_EVERY = 100, 500\nLR = 1e-3\n\n# Constraint mode (kept for hyperparameter compatibility)：'hard' or 'soft' (本实现不依赖此开关，但保持一致性)\nBC_MODE = 'hard'\nUSE_HARD_BC = BC_MODE == 'hard'\nLAM_BC = 1.0                         # 软约束下的边界损失权重 λ_bc\n# 下面这些在当前“最传统 PINN”里不用，但按你的要求保留\nS0, S1, kappa = 5.0, 7.0, 2.0\nlam_comp = 0.05\ntau, eta_phi = 0.1, 1e-4\nlam_pde, lam_phi = 1.0, 0.01        # 这里我们会用到 lam_pde (PDE residual权重)\n\n\n# ======================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\ndef b_fun(x): return torch.ones_like(x)\ndef c_fun(x): return torch.zeros_like(x)\ndef f_fun(x): return torch.exp(x)\n\ndef exact_solution(x_np, eps):\n    import numpy as np\n    x = np.asarray(x_np, dtype=np.float64)\n    a = 1.0/eps\n\n    # stable ratio R(x) in [0,1]\n    # R(x) = (e^{a x}-1)/(e^{a}-1) = e^{a(x-1)} * (1 - e^{-a x})/(1 - e^{-a})\n    exp_neg_a = np.exp(-a)\n    numerator = np.exp(a*(x-1.0)) * (1.0 - np.exp(-a*x))\n    denominator = 1.0 - exp_neg_a\n    R = numerator / denominator\n\n    # compact/stable closed form:\n    # u(x) = [ e^{x} - ε + (ε - e) * R(x) ] / (1 - ε)\n    u = (np.exp(x) - eps + (eps - np.e) * R) / (1.0 - eps)\n    return u.astype(np.float64)\n    \n\n# Example B: left boundary layer\nLAYER_SIDE = 'left'; BC_LEFT = ('dirichlet', 1.0); BC_RIGHT = ('dirichlet', 0.0)\nEPS_LIST = [5e-2, 1e-2, 5e-3, 1e-3]\ntrain(EPS_LIST, layer_side=LAYER_SIDE, bc_left=BC_LEFT, bc_right=BC_RIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f2275",
   "metadata": {},
   "outputs": [],
   "source": [
    "\ntrain(\n    EPS_LIST,\n    layer_side=LAYER_SIDE,\n    bc_left=BC_LEFT,\n    bc_right=BC_RIGHT\n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac54dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDE:  ε y'' + y' + 2e^{-x} = 0   ==>  -ε y'' - y' - 2e^{-x} = 0\ndef b_fun(x): return -torch.ones_like(x)        # b = -1\ndef c_fun(x): return torch.zeros_like(x)        # c = 0\ndef f_fun(x): return 2.0 * torch.exp(-x)        # f = 2 e^{-x}\n\ndef exact_solution(x_np, eps):\n    x = np.asarray(x_np, dtype=np.float64)\n    return 2.0*np.exp(-x) - 2.0*eps*np.exp(-x/eps)\n\n# Left boundary layer (b<0)\nLAYER_SIDE = 'left'\n\n# Left Neumann (q0=0), Right Dirichlet (β=2e^{-1})\nBC_LEFT  = ('neumann', 0.0)\nBC_RIGHT = ('dirichlet', float(2.0*np.exp(-1.0)))\n\n# Your homotopy parameters\nEPS_LIST = [5e-4, 1e-4, 5e-5, 1e-5]\ntrain(EPS_LIST, layer_side=LAYER_SIDE, bc_left=BC_LEFT, bc_right=BC_RIGHT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3p8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}